*** SLURM BATCH JOB 'test_job' STARTING ***
*** Activating environment AD_Project ***
/home/dana.gershon/project/DL_Project/EvaluatorClass.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(self.model_path, map_location=self.device))
/home/dana.gershon/project/DL_Project/EvaluatorClass.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  latent_params = torch.load(self.latent_path, map_location=self.device)
/home/dana.gershon/project/DL_Project/EvaluatorClass.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(self.model_path, map_location=self.device))
/home/dana.gershon/project/DL_Project/EvaluatorClass.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  latent_params = torch.load(self.latent_path, map_location=self.device)
Epoch [1/100], Loss: 0.5147
Epoch [2/100], Loss: 0.4883
Epoch [3/100], Loss: 0.4748
Epoch [4/100], Loss: 0.4457
Epoch [5/100], Loss: 0.4074
Epoch [6/100], Loss: 0.3720
Epoch [7/100], Loss: 0.3600
Epoch [8/100], Loss: 0.3768
Epoch [9/100], Loss: 0.3607
Epoch [10/100], Loss: 0.3368
Epoch [11/100], Loss: 0.3256
Epoch [12/100], Loss: 0.3202
Epoch [13/100], Loss: 0.3168
Epoch [14/100], Loss: 0.3168
Epoch [15/100], Loss: 0.3182
Epoch [16/100], Loss: 0.3201
Epoch [17/100], Loss: 0.3183
Epoch [18/100], Loss: 0.3145
Epoch [19/100], Loss: 0.3099
Epoch [20/100], Loss: 0.3038
Epoch [21/100], Loss: 0.2983
Epoch [22/100], Loss: 0.2931
Epoch [23/100], Loss: 0.2878
Epoch [24/100], Loss: 0.2848
Epoch [25/100], Loss: 0.2835
Epoch [26/100], Loss: 0.2825
Epoch [27/100], Loss: 0.2820
Epoch [28/100], Loss: 0.2814
Epoch [29/100], Loss: 0.2816
Epoch [30/100], Loss: 0.2813
Epoch [31/100], Loss: 0.2809
Epoch [32/100], Loss: 0.2818
Epoch [33/100], Loss: 0.2813
Epoch [34/100], Loss: 0.2811
Epoch [35/100], Loss: 0.2809
Epoch [36/100], Loss: 0.2802
Epoch [37/100], Loss: 0.2790
Epoch [38/100], Loss: 0.2781
Epoch [39/100], Loss: 0.2774
Epoch [40/100], Loss: 0.2761
Epoch [41/100], Loss: 0.2754
Epoch [42/100], Loss: 0.2747
Epoch [43/100], Loss: 0.2740
Epoch [44/100], Loss: 0.2728
Epoch [45/100], Loss: 0.2716
Epoch [46/100], Loss: 0.2710
Epoch [47/100], Loss: 0.2698
Epoch [48/100], Loss: 0.2691
Epoch [49/100], Loss: 0.2682
Epoch [50/100], Loss: 0.2669
Epoch [51/100], Loss: 0.2660
Epoch [52/100], Loss: 0.2651
Epoch [53/100], Loss: 0.2644
Epoch [54/100], Loss: 0.2641
Epoch [55/100], Loss: 0.2639
Epoch [56/100], Loss: 0.2639
Epoch [57/100], Loss: 0.2636
Epoch [58/100], Loss: 0.2632
Epoch [59/100], Loss: 0.2631
Epoch [60/100], Loss: 0.2631
Epoch [61/100], Loss: 0.2633
Epoch [62/100], Loss: 0.2633
Epoch [63/100], Loss: 0.2630
Epoch [64/100], Loss: 0.2628
Epoch [65/100], Loss: 0.2625
Epoch [66/100], Loss: 0.2622
Epoch [67/100], Loss: 0.2615
Epoch [68/100], Loss: 0.2613
Epoch [69/100], Loss: 0.2611
Epoch [70/100], Loss: 0.2608
Epoch [71/100], Loss: 0.2608
Epoch [72/100], Loss: 0.2604
Epoch [73/100], Loss: 0.2603
Epoch [74/100], Loss: 0.2600
Epoch [75/100], Loss: 0.2599
Epoch [76/100], Loss: 0.2598
Epoch [77/100], Loss: 0.2596
Epoch [78/100], Loss: 0.2595
Epoch [79/100], Loss: 0.2592
Epoch [80/100], Loss: 0.2591
Epoch [81/100], Loss: 0.2587
Epoch [82/100], Loss: 0.2587
Epoch [83/100], Loss: 0.2584
Epoch [84/100], Loss: 0.2584
Epoch [85/100], Loss: 0.2582
Epoch [86/100], Loss: 0.2579
Epoch [87/100], Loss: 0.2578
Epoch [88/100], Loss: 0.2578
Epoch [89/100], Loss: 0.2576
Epoch [90/100], Loss: 0.2577
Epoch [91/100], Loss: 0.2577
Epoch [92/100], Loss: 0.2578
Epoch [93/100], Loss: 0.2577
Epoch [94/100], Loss: 0.2576
Epoch [95/100], Loss: 0.2575
Epoch [96/100], Loss: 0.2573
Epoch [97/100], Loss: 0.2572
Epoch [98/100], Loss: 0.2573
Epoch [99/100], Loss: 0.2571
Epoch [100/100], Loss: 0.2569
Evaluating on Training Set...
Training Set Loss: 0.2545
Evaluating on Test Set...
Epoch [1/20], Test Set Loss: 0.7252
Epoch [2/20], Test Set Loss: 0.5844
Epoch [3/20], Test Set Loss: 0.4990
Epoch [4/20], Test Set Loss: 0.4434
Epoch [5/20], Test Set Loss: 0.4071
Epoch [6/20], Test Set Loss: 0.3827
Epoch [7/20], Test Set Loss: 0.3657
Epoch [8/20], Test Set Loss: 0.3535
Epoch [9/20], Test Set Loss: 0.3443
Epoch [10/20], Test Set Loss: 0.3370
Epoch [11/20], Test Set Loss: 0.3310
Epoch [12/20], Test Set Loss: 0.3260
Epoch [13/20], Test Set Loss: 0.3218
Epoch [14/20], Test Set Loss: 0.3181
Epoch [15/20], Test Set Loss: 0.3148
Epoch [16/20], Test Set Loss: 0.3118
Epoch [17/20], Test Set Loss: 0.3092
Epoch [18/20], Test Set Loss: 0.3067
Epoch [19/20], Test Set Loss: 0.3046
Epoch [20/20], Test Set Loss: 0.3026
Test Set Loss: 0.3795
Epoch [1/100], Loss: 47.7450
Epoch [2/100], Loss: 47.6603
Epoch [3/100], Loss: 47.6080
Epoch [4/100], Loss: 47.5538
Epoch [5/100], Loss: 47.5000
Epoch [6/100], Loss: 47.4459
Epoch [7/100], Loss: 47.3916
Epoch [8/100], Loss: 47.3374
Epoch [9/100], Loss: 47.2831
Epoch [10/100], Loss: 47.2292
Epoch [11/100], Loss: 47.1752
Epoch [12/100], Loss: 47.1213
Epoch [13/100], Loss: 47.0675
Epoch [14/100], Loss: 47.0134
Epoch [15/100], Loss: 46.9598
Epoch [16/100], Loss: 46.9059
Epoch [17/100], Loss: 46.8519
Epoch [18/100], Loss: 46.7989
Epoch [19/100], Loss: 46.7459
Epoch [20/100], Loss: 46.6922
Epoch [21/100], Loss: 46.6394
Epoch [22/100], Loss: 46.5861
Epoch [23/100], Loss: 46.5333
Epoch [24/100], Loss: 46.4802
Epoch [25/100], Loss: 46.4273
Epoch [26/100], Loss: 46.3744
Epoch [27/100], Loss: 46.3216
Epoch [28/100], Loss: 46.2698
Epoch [29/100], Loss: 46.2169
Epoch [30/100], Loss: 46.1648
Epoch [31/100], Loss: 46.1122
Epoch [32/100], Loss: 46.0601
Epoch [33/100], Loss: 46.0079
Epoch [34/100], Loss: 45.9560
Epoch [35/100], Loss: 45.9041
Epoch [36/100], Loss: 45.8523
Epoch [37/100], Loss: 45.8006
Epoch [38/100], Loss: 45.7487
Epoch [39/100], Loss: 45.6974
Epoch [40/100], Loss: 45.6458
Epoch [41/100], Loss: 45.5939
Epoch [42/100], Loss: 45.5431
Epoch [43/100], Loss: 45.4915
Epoch [44/100], Loss: 45.4406
Epoch [45/100], Loss: 45.3895
Epoch [46/100], Loss: 45.3388
Epoch [47/100], Loss: 45.2876
Epoch [48/100], Loss: 45.2366
Epoch [49/100], Loss: 45.1860
Epoch [50/100], Loss: 45.1351
Epoch [51/100], Loss: 45.0846
Epoch [52/100], Loss: 45.0340
Epoch [53/100], Loss: 44.9835
Epoch [54/100], Loss: 44.9332
Epoch [55/100], Loss: 44.8829
Epoch [56/100], Loss: 44.8327
Epoch [57/100], Loss: 44.7826
Epoch [58/100], Loss: 44.7320
Epoch [59/100], Loss: 44.6825
Epoch [60/100], Loss: 44.6324
Epoch [61/100], Loss: 44.5823
Epoch [62/100], Loss: 44.5330
Epoch [63/100], Loss: 44.4829
Epoch [64/100], Loss: 44.4332
Epoch [65/100], Loss: 44.3838
Epoch [66/100], Loss: 44.3339
Epoch [67/100], Loss: 44.2847
Epoch [68/100], Loss: 44.2354
Epoch [69/100], Loss: 44.1862
Epoch [70/100], Loss: 44.1365
Epoch [71/100], Loss: 44.0873
Epoch [72/100], Loss: 44.0384
Epoch [73/100], Loss: 43.9893
Epoch [74/100], Loss: 43.9401
Epoch [75/100], Loss: 43.8914
Epoch [76/100], Loss: 43.8428
Epoch [77/100], Loss: 43.7939
Epoch [78/100], Loss: 43.7449
Epoch [79/100], Loss: 43.6964
Epoch [80/100], Loss: 43.6479
Epoch [81/100], Loss: 43.5987
Epoch [82/100], Loss: 43.5506
Epoch [83/100], Loss: 43.5023
Epoch [84/100], Loss: 43.4534
Epoch [85/100], Loss: 43.4056
Epoch [86/100], Loss: 43.3575
Epoch [87/100], Loss: 43.3090
Epoch [88/100], Loss: 43.2610
Epoch [89/100], Loss: 43.2129
Epoch [90/100], Loss: 43.1652
Epoch [91/100], Loss: 43.1170
Epoch [92/100], Loss: 43.0694
Epoch [93/100], Loss: 43.0212
Epoch [94/100], Loss: 42.9737
Epoch [95/100], Loss: 42.9258
Epoch [96/100], Loss: 42.8780
Epoch [97/100], Loss: 42.8303
Epoch [98/100], Loss: 42.7830
Epoch [99/100], Loss: 42.7354
Epoch [100/100], Loss: 42.6879
Evaluating on Training Set...
Training Set Loss: 43.6613
Evaluating on Test Set...
Epoch [1/20], Test Set Loss: 48.8600
Epoch [2/20], Test Set Loss: 48.3212
Epoch [3/20], Test Set Loss: 47.8012
Epoch [4/20], Test Set Loss: 47.2806
Epoch [5/20], Test Set Loss: 46.7629
Epoch [6/20], Test Set Loss: 46.2520
Epoch [7/20], Test Set Loss: 45.7466
Epoch [8/20], Test Set Loss: 45.2485
Epoch [9/20], Test Set Loss: 44.7564
Epoch [10/20], Test Set Loss: 44.2706
Epoch [11/20], Test Set Loss: 43.7913
Epoch [12/20], Test Set Loss: 43.3174
Epoch [13/20], Test Set Loss: 42.8488
Epoch [14/20], Test Set Loss: 42.3851
Epoch [15/20], Test Set Loss: 41.9272
Epoch [16/20], Test Set Loss: 41.4736
Epoch [17/20], Test Set Loss: 41.0246
Epoch [18/20], Test Set Loss: 40.5802
Epoch [19/20], Test Set Loss: 40.1398
Epoch [20/20], Test Set Loss: 39.7039
Test Set Loss: 44.1246
*** SLURM BATCH JOB 'test_job' DONE ***
