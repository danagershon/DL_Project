*** SLURM BATCH JOB 'test_job' STARTING ***
*** Activating environment AD_Project ***
/home/dana.gershon/project/DL_Project/EvaluatorClass.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(self.model_path, map_location=self.device))
/home/dana.gershon/project/DL_Project/EvaluatorClass.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  latents = torch.load(self.latent_path, map_location=self.device)
Epoch [1/100], Loss: 0.5260
Epoch [2/100], Loss: 0.4907
Epoch [3/100], Loss: 0.4776
Epoch [4/100], Loss: 0.4476
Epoch [5/100], Loss: 0.4068
Epoch [6/100], Loss: 0.3731
Epoch [7/100], Loss: 0.3681
Epoch [8/100], Loss: 0.3845
Epoch [9/100], Loss: 0.3545
Epoch [10/100], Loss: 0.3381
Epoch [11/100], Loss: 0.3304
Epoch [12/100], Loss: 0.3289
Epoch [13/100], Loss: 0.3297
Epoch [14/100], Loss: 0.3298
Epoch [15/100], Loss: 0.3252
Epoch [16/100], Loss: 0.3186
Epoch [17/100], Loss: 0.3102
Epoch [18/100], Loss: 0.3030
Epoch [19/100], Loss: 0.2989
Epoch [20/100], Loss: 0.2944
Epoch [21/100], Loss: 0.2908
Epoch [22/100], Loss: 0.2881
Epoch [23/100], Loss: 0.2860
Epoch [24/100], Loss: 0.2856
Epoch [25/100], Loss: 0.2849
Epoch [26/100], Loss: 0.2856
Epoch [27/100], Loss: 0.2854
Epoch [28/100], Loss: 0.2851
Epoch [29/100], Loss: 0.2850
Epoch [30/100], Loss: 0.2845
Epoch [31/100], Loss: 0.2853
Epoch [32/100], Loss: 0.2849
Epoch [33/100], Loss: 0.2847
Epoch [34/100], Loss: 0.2839
Epoch [35/100], Loss: 0.2827
Epoch [36/100], Loss: 0.2817
Epoch [37/100], Loss: 0.2803
Epoch [38/100], Loss: 0.2791
Epoch [39/100], Loss: 0.2777
Epoch [40/100], Loss: 0.2763
Epoch [41/100], Loss: 0.2751
Epoch [42/100], Loss: 0.2741
Epoch [43/100], Loss: 0.2732
Epoch [44/100], Loss: 0.2721
Epoch [45/100], Loss: 0.2707
Epoch [46/100], Loss: 0.2692
Epoch [47/100], Loss: 0.2677
Epoch [48/100], Loss: 0.2667
Epoch [49/100], Loss: 0.2659
Epoch [50/100], Loss: 0.2653
Epoch [51/100], Loss: 0.2646
Epoch [52/100], Loss: 0.2640
Epoch [53/100], Loss: 0.2636
Epoch [54/100], Loss: 0.2632
Epoch [55/100], Loss: 0.2629
Epoch [56/100], Loss: 0.2627
Epoch [57/100], Loss: 0.2625
Epoch [58/100], Loss: 0.2625
Epoch [59/100], Loss: 0.2625
Epoch [60/100], Loss: 0.2624
Epoch [61/100], Loss: 0.2627
Epoch [62/100], Loss: 0.2629
Epoch [63/100], Loss: 0.2627
Epoch [64/100], Loss: 0.2627
Epoch [65/100], Loss: 0.2634
Epoch [66/100], Loss: 0.2638
Epoch [67/100], Loss: 0.2640
Epoch [68/100], Loss: 0.2639
Epoch [69/100], Loss: 0.2637
Early stopping at epoch 70, best loss: 0.2624
Evaluating on Training Set...
Training Set Loss: 0.2600
Evaluating on Test Set...
Epoch [1/20], Test Set Loss: 0.7211
Epoch [2/20], Test Set Loss: 0.5737
Epoch [3/20], Test Set Loss: 0.4886
Epoch [4/20], Test Set Loss: 0.4367
Epoch [5/20], Test Set Loss: 0.4046
Epoch [6/20], Test Set Loss: 0.3832
Epoch [7/20], Test Set Loss: 0.3678
Epoch [8/20], Test Set Loss: 0.3562
Epoch [9/20], Test Set Loss: 0.3470
Epoch [10/20], Test Set Loss: 0.3397
Epoch [11/20], Test Set Loss: 0.3336
Epoch [12/20], Test Set Loss: 0.3284
Epoch [13/20], Test Set Loss: 0.3241
Epoch [14/20], Test Set Loss: 0.3203
Epoch [15/20], Test Set Loss: 0.3168
Epoch [16/20], Test Set Loss: 0.3138
Epoch [17/20], Test Set Loss: 0.3110
Epoch [18/20], Test Set Loss: 0.3087
Epoch [19/20], Test Set Loss: 0.3064
Epoch [20/20], Test Set Loss: 0.3044
Test Set Loss: 0.3793
*** SLURM BATCH JOB 'test_job' DONE ***
